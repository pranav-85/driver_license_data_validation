{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3da8a847-6519-4625-8d96-9b8e92680044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cfc4984-ba18-45c8-8686-903737fd3478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"jJyjVbMQRppa3lA1JR0i\")\n",
    "project = rf.workspace().project(\"driver-s-license-text-extraction-6zjsk\")\n",
    "model = project.version(1).model\n",
    "\n",
    "# infer on a local image\n",
    "pred_data = model.predict(\"sample.jpg\", confidence=40, overlap=30).json()\n",
    "\n",
    "# visualize your prediction\n",
    "model.predict(\"sample.jpg\", confidence=40, overlap=30).save(\"prediction.jpg\")\n",
    "\n",
    "# infer on an image hosted elsewhere\n",
    "# print(model.predict(\"URL_OF_YOUR_IMAGE\", hosted=True, confidence=40, overlap=30).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06d4dada-e50f-45fb-8c0f-e22f684af4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread('sample.jpg')\n",
    "\n",
    "# Convert the image to grayscale\n",
    "grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Save the grayscale image\n",
    "cv2.imwrite('sample_grayscale.jpg', grayscale_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44c535f5-4917-45e5-85b7-b6c04e038137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cropped image: cropped_images\\Exp date_1.jpg\n",
      "Saved cropped image: cropped_images\\First name_2.jpg\n",
      "Saved cropped image: cropped_images\\Last name_3.jpg\n",
      "Saved cropped image: cropped_images\\Issue date_4.jpg\n",
      "Saved cropped image: cropped_images\\Address_5.jpg\n",
      "Saved cropped image: cropped_images\\DOB_6.jpg\n",
      "Saved cropped image: cropped_images\\Sex_7.jpg\n",
      "Saved cropped image: cropped_images\\Class_8.jpg\n",
      "Saved cropped image: cropped_images\\License number_9.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def adjust_black_and_white(image_path, threshold=50):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded correctly\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None  # Exit the function if image loading fails\n",
    "\n",
    "    # Create a mask for pixels close to black (i.e., pixels where all channels are close to 0)\n",
    "    black_mask = np.all(image <= threshold, axis=-1)\n",
    "\n",
    "    # Create a mask for pixels close to white (i.e., pixels where all channels are close to 255)\n",
    "    white_mask = np.all(image >= 255 - threshold, axis=-1)\n",
    "\n",
    "    # Set pixels close to black to pure black\n",
    "    image[black_mask] = [0, 0, 0]\n",
    "\n",
    "    # Set pixels close to white to pure white\n",
    "    image[white_mask] = [255, 255, 255]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# Function to crop the image based on midpoint coordinates\n",
    "def crop_image_by_midpoints(image_path, coordinates, output_dir):\n",
    "    # Load the image using OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Iterate over the coordinates and crop the image\n",
    "    for i, coord in enumerate(coordinates):\n",
    "        # Extract the midpoint coordinates (x, y), and width, height\n",
    "        x = coord['x']\n",
    "        y = coord['y']\n",
    "        width = coord['width']\n",
    "        height = coord['height']\n",
    "\n",
    "        extra = 0\n",
    "        # Calculate the top-left and bottom-right corners based on the midpoint and size\n",
    "        x1 = int(x - width / 2) -extra\n",
    "        y1 = int(y - height / 2)  - extra\n",
    "        x2 = int(x + width / 2) + extra\n",
    "        y2 = int(y + height / 2) + extra\n",
    "\n",
    "        # Ensure that the coordinates are within the image boundaries\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(image.shape[1], x2)\n",
    "        y2 = min(image.shape[0], y2)\n",
    "\n",
    "        # Crop the image using NumPy array slicing\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "        # Save the cropped image\n",
    "        output_image_path = os.path.join(output_dir, f\"{coord['class']}_{i+1}.jpg\")\n",
    "        cv2.imwrite(output_image_path, cropped_image)\n",
    "        print(f\"Saved cropped image: {output_image_path}\")\n",
    "\n",
    "        image_path = output_image_path  # Make sure this path is correct!\n",
    "\n",
    "        # Adjust pixels close to black and white\n",
    "        adjusted_image = adjust_black_and_white(image_path)\n",
    "\n",
    "        if adjusted_image is not None:\n",
    "        # Save the resulting image\n",
    "            output_path = output_image_path\n",
    "            cv2.imwrite(output_path, adjusted_image)\n",
    "\n",
    "# Example coordinates (replace this with your actual annotation data)\n",
    "\n",
    "# Path to the image file\n",
    "image_path = 'sample_grayscale.jpg'\n",
    "\n",
    "# Directory to save cropped images\n",
    "output_dir = 'cropped_images'\n",
    "\n",
    "# Call the function to crop the image based on midpoint coordinates\n",
    "crop_image_by_midpoints(image_path, pred_data['predictions'], output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09644a25-f27e-4839-8d4c-5ea08f6287ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Address\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: Class\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: DOB\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: Exp date\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: First name\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: Issue date\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: Last name\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: License number\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n",
      "Class: Sex\n",
      "Extracted Text: \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to Tesseract executable (if it's not in your PATH, you'll need to specify the full path)\n",
    "# Example for Windows: pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Function to preprocess and perform OCR on each image in the cropped_images directory\n",
    "def preprocess_image(image_path, alpha=1.5, beta=0):\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Adjust contrast by modifying alpha and beta\n",
    "    contrast_img = cv2.convertScaleAbs(gray_img, alpha=alpha, beta=beta)\n",
    "\n",
    "    # Apply thresholding to binarize the image (you can experiment with different methods)\n",
    "    _, binary_img = cv2.threshold(contrast_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Optional: Apply morphological transformations to clean the image (if needed)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    morph_img = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Return the processed image\n",
    "    return Image.fromarray(morph_img)\n",
    "\n",
    "# Function to perform OCR on each image in the cropped_images directory\n",
    "def ocr_on_cropped_images(cropped_images_dir):\n",
    "    # Iterate over all files in the cropped_images directory\n",
    "    for filename in os.listdir(cropped_images_dir):\n",
    "        # Only process image files (you can filter by extensions like .jpg, .jpeg, .png)\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(cropped_images_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                # Preprocess the image (change contrast here)\n",
    "                processed_img = preprocess_image(image_path, alpha=1.5, beta=0)\n",
    "                \n",
    "                # Use pytesseract to extract text\n",
    "                text = pytesseract.image_to_string(processed_img)\n",
    "\n",
    "                # Extract the class name from the image filename\n",
    "                # Assuming the class is part of the filename (e.g., 'cropped_License Number_1.jpg')\n",
    "                class_name = filename.split('_')[0]  # This will give the class name between 'cropped' and the index\n",
    "\n",
    "                # Display the extracted text and class name\n",
    "                print(f\"Class: {class_name}\")\n",
    "                print(f\"Extracted Text: {text.strip()}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue  # Skip the problematic image and move to the next\n",
    "\n",
    "# Path to the cropped_images directory\n",
    "cropped_images_dir = 'cropped_images'\n",
    "\n",
    "# Call the function to perform OCR on the cropped images\n",
    "ocr_on_cropped_images(cropped_images_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ef332ad-594c-4338-b30f-09efa7958ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install easyocr==1.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f6a046f-2f58-4f97-96a7-f1bc97127b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a15000a-40ce-481b-b5a1-1c904ae84695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90013463-0465-4f79-b9b1-e7023e1a965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b5305d-9074-4ae1-b733-a44b76512785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1f7f288-dcbc-4853-b9c3-260422762404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99da75ff-12e5-4de2-a552-33fed9ff573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1bed4e9-e1c3-4bfe-bdf7-20ff276a9c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.46.2\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": false,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Address\n",
      "Extracted Text: MANCHESTER, NET03111\n",
      "--------------------------------------------------\n",
      "Class: Class\n",
      "Extracted Text: CHILD\n",
      "--------------------------------------------------\n",
      "Class: DOB\n",
      "Extracted Text: 05/23/2016\n",
      "--------------------------------------------------\n",
      "Class: Exp date\n",
      "Extracted Text: 05/23/2032\n",
      "--------------------------------------------------\n",
      "Class: First name\n",
      "Extracted Text: VALENTINA\n",
      "--------------------------------------------------\n",
      "Class: Issue date\n",
      "Extracted Text: 06/05/2021\n",
      "--------------------------------------------------\n",
      "Class: Last name\n",
      "Extracted Text: VEGA\n",
      "--------------------------------------------------\n",
      "Class: License number\n",
      "Extracted Text: 603-333-3333\n",
      "--------------------------------------------------\n",
      "Class: Sex\n",
      "Extracted Text: E\n",
      "--------------------------------------------------\n",
      "\n",
      "Extracted Text Dictionary Class-Wise:\n",
      "Class: Address\n",
      "  Extracted Text: MANCHESTER, NET03111\n",
      "--------------------------------------------------\n",
      "Class: Class\n",
      "  Extracted Text: CHILD\n",
      "--------------------------------------------------\n",
      "Class: DOB\n",
      "  Extracted Text: 05/23/2016\n",
      "--------------------------------------------------\n",
      "Class: Exp date\n",
      "  Extracted Text: 05/23/2032\n",
      "--------------------------------------------------\n",
      "Class: First name\n",
      "  Extracted Text: VALENTINA\n",
      "--------------------------------------------------\n",
      "Class: Issue date\n",
      "  Extracted Text: 06/05/2021\n",
      "--------------------------------------------------\n",
      "Class: Last name\n",
      "  Extracted Text: VEGA\n",
      "--------------------------------------------------\n",
      "Class: License number\n",
      "  Extracted Text: 603-333-3333\n",
      "--------------------------------------------------\n",
      "Class: Sex\n",
      "  Extracted Text: E\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "extract_classes = ['First name', 'Last name', 'License number', 'Exp date']\n",
    "\n",
    "data = {}\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(cropped_images_dir):\n",
    "    print(f\"Error: The directory {cropped_images_dir} does not exist.\")\n",
    "else:\n",
    "    # Load the TrOCR model and processor\n",
    "    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "\n",
    "    # Initialize a dictionary to store the extracted text class-wise\n",
    "    extracted_text_dict = {}\n",
    "\n",
    "    # Loop over each image file in the directory\n",
    "    for filename in os.listdir(cropped_images_dir):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(cropped_images_dir, filename)\n",
    "            \n",
    "            try:\n",
    "                # Open the image\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                \n",
    "                # Process the image\n",
    "                pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "                # Generate text prediction\n",
    "                generated_ids = model.generate(pixel_values, max_new_tokens=100)\n",
    "                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "                # Extract class name from filename (modify as needed)\n",
    "                class_name = filename.split('_')[0]  # Adjust if needed for different file naming\n",
    "\n",
    "                # Store the extracted text in the dictionary\n",
    "                if class_name not in extracted_text_dict:\n",
    "                    extracted_text_dict[class_name] = []\n",
    "                extracted_text_dict[class_name].append(generated_text.strip())\n",
    "\n",
    "                # Optionally, print the result to the console\n",
    "                print(f\"Class: {class_name}\")\n",
    "                print(f\"Extracted Text: {generated_text.strip()}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # After all images are processed, print the dictionary\n",
    "    print(\"\\nExtracted Text Dictionary Class-Wise:\")\n",
    "    for class_name, texts in extracted_text_dict.items():\n",
    "        print(f\"Class: {class_name}\")\n",
    "        if class_name in extract_classes:\n",
    "            data[class_name] = \"\"\n",
    "        for text in texts:\n",
    "            print(f\"  Extracted Text: {text}\")\n",
    "            if class_name in extract_classes:\n",
    "                data[class_name] += text\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b30e0a6-5759-458f-8111-a55a7681bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fbb50f4-7000-4c32-8a93-be8e3e428fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall transformers sentencepiece -y\n",
    "# !pip install transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0caeb0a6-c06e-4f66-b80c-eff2c0fde546",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Name'] = data['First name'] +' ' + data['Last name']\n",
    "del data['First name']\n",
    "del data['Last name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "889d41bd-6ca8-49a4-a8d3-d461e36ef285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Exp date': '05/23/2032',\n",
       " 'License number': '603-333-3333',\n",
       " 'Name': 'VALENTINA VEGA'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34f29723-b7e0-48dd-b020-4fe158ea8d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cropped_images' has been removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(cropped_images_dir) and os.path.isdir(cropped_images_dir):\n",
    "    try:\n",
    "        # Remove the directory and all of its contents\n",
    "        shutil.rmtree(cropped_images_dir)\n",
    "        print(f\"'{cropped_images_dir}' has been removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while removing the directory: {e}\")\n",
    "else:\n",
    "    print(f\"The directory '{cropped_images_dir}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275611a-cdca-497b-a9bd-76c75084f8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "license_det",
   "language": "python",
   "name": "license_det"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
